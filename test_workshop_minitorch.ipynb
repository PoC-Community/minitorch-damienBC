import numpy as np
from typing import Any, Callable

class MyCrossBackward:

    def __init__(self, *args, backward_function : Callable[..., None], name : str) -> None:
        self.__data = list(args)
        self.__bf = backward_function
        self.__name = name

    def __str__(self) -> str:
        return self.__name
    
    def __call__(self, x) -> Any:
        return self.run(x)
    
    def run(self, base) -> None:
        return self.__bf(base, *self.__data)

class MyTensor:

    def __init__(self,
                 data : list | np.ndarray | int | float,
                 dtype : np.dtype = None,
                 requires_grad : bool = False,
                 crossOpBack : MyCrossBackward = None) -> None:
        self.__data = np.array(data, dtype=dtype)
        if (isinstance(data, (int, float))):
            self.__data = np.array([data])
        self.dtype = self.__data.dtype if dtype is None else dtype
        self.requires_grad = requires_grad
        self.grad = None
        self.backward_op = crossOpBack

    @property
    def shape(self):
        return self.__data.shape

    @property
    def strides(self):
        return self.__data.strides
    
    @property
    def data(self):
        return self.__data

    @property
    def size(self):
        return self.__data.size

    def __str__(self) -> str:
        return f"Tensor of shape {self.shape} and dtype {self.dtype}"

    def __repr__(self) -> str:
        return self.__str__()

    def __getitem__(self, index):
        return self.__data[index]
    
    def __setitem__(self, index, value):
        self.__data[index] = value

    def __add__(self, x):
        return MyTensor(self.__data + x.data,
                             requires_grad=x.requires_grad or self.requires_grad,
                             crossOpBack=MyCrossBackward(
                                self, x,
                                backward_function=add_deriv,
                                name="AddBackward"
                             ) if x.requires_grad or self.requires_grad else None)

    def __mul__(self, other):
        return MyTensor(
            self.__data * other.data,
            dtype=self.dtype,
            requires_grad=self.requires_grad,
            crossOpBack=MyCrossBackward(
                self,
                other,
                backward_function=mul_deriv,
                name="mul"
            )
        )

    def __pow__(self, other):
        return MyTensor(
            self.__data ** other.data,
            dtype=self.dtype,
            requires_grad=self.requires_grad,
            crossOpBack=MyCrossBackward(
                self,
                other,
                backward_function=pow_deriv,
                name="pow"
            )
        )

    def __truediv__(self, other):
        return MyTensor(
            self.__data / other.data,
            dtype=self.dtype,
            requires_grad=self.requires_grad,
            crossOpBack=MyCrossBackward(
                self,
                other,
                backward_function=div_deriv,
                name="div"
            )
        )

    def backward(self, base = None) -> None:
        if base is None:
            base = MyTensor(1, dtype=self.dtype)

        if self.backward_op != None:
            self.backward_op(base)

def add_deriv(base, *tensors):
    left : MyTensor = tensors[0]
    right : MyTensor = tensors[1]

    if (left.requires_grad):
        grad = MyTensor(base.data.T * np.ones_like(right.data))
        if (left.backward_op == None):
            left.grad = left.grad and left.grad + grad or grad 
        left.backward(grad)
    
    if (right.requires_grad):
        grad = MyTensor(base.data.T * np.ones_like(left.data))
        if (right.backward_op == None):
            right.grad = right.grad and right.grad + grad or grad
        right.backward(grad)

def mul_deriv(base, *tensors):
    left : MyTensor = tensors[0]
    right : MyTensor = tensors[1]

    if (left.requires_grad):
        grad = MyTensor(base.data.T * right.data)
        if (left.backward_op == None):
            left.grad = left.grad and left.grad + grad or grad 
        left.backward(grad)
    
    if (right.requires_grad):
        grad = MyTensor(base.data.T * left.data)
        if (right.backward_op == None):
            right.grad = right.grad and right.grad + grad or grad
        right.backward(grad)

def div_deriv(base, *tensors):
    left : MyTensor = tensors[0]
    right : MyTensor = tensors[1]

    if (left.requires_grad):
        grad = MyTensor(base.data.T / right.data)
        if (left.backward_op == None):
            left.grad = left.grad and left.grad + grad or grad 
        left.backward(grad)
    
    if (right.requires_grad):
        grad = MyTensor(base.data.T * left.data / right.data ** 2)
        if (right.backward_op == None):
            right.grad = right.grad and right.grad + grad or grad
        right.backward(grad)

def pow_deriv(base, *args):
    left : MyTensor = args[0]
    right : MyTensor = args[1]

    if (left.requires_grad):
        grad = MyTensor(base.data * right.data * left.data ** (right.data - 1))
        if (left.backward_op == None):
            left.grad = left.grad and left.grad + grad or grad 
        left.backward(grad)
    
    if (right.requires_grad):
        grad = MyTensor(base.data * np.log(left.data) * left.data ** right.data)
        if (right.backward_op == None):
            right.grad = right.grad and right.grad + grad or grad
        right.backward(grad)

def sub_deriv(base, *tensors):
    left : MyTensor = tensors[0]
    right : MyTensor = tensors[1]

    if (left.requires_grad):
        grad = MyTensor(base.data * np.ones_like(right.data))
        if (left.backward_op == None):
            left.grad = left.grad and left.grad + grad or grad 
        left.backward(grad)
    
    if (right.requires_grad):
        grad = MyTensor(base.data * -np.ones_like(left.data))
        if (right.backward_op == None):
            right.grad = right.grad and right.grad + grad or grad
        right.backward(grad)

def test_tensor_class(TensorType):
    a = TensorType([1, 2, 3, 4, 5])
    assert (hasattr(a, "__add__") and hasattr(a, "__mul__") and hasattr(a, "__truediv__") and hasattr(a, "__pow__") and hasattr(a, "__sub__")), "Missing operations. The operations to implement: add, div, pow, mul, sub"
    b = TensorType([3, 4, 2, 5], requires_grad=True)
    assert (b.requires_grad == True), "requires_grad option does not function properly"

test_tensor_class(MyTensor)
